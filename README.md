# Top 'n' Similarity Recommendation

## Objective
The primary objective of this project is to develop a similarity comparison algorithm capable of listing similar Type#1s and Type#2s for each Type#1/Type#2. The project emphasizes performance and production readiness to accommodate the substantial amount of data to be processed multiple times a day.

## Introduction
The project revolves around a vast database housing over 800k records of web-scraped Type#1s and Type#2s. Each Type#1/Type#2 entry contains a plethora of information, including textual details like Type#1 text/title, categorical attributes such as Type#1 type or buyer, and continuous variables like price or execution deadline days. Additionally, Type#1s and Type#2s are categorized based on European procurement Category#1 codes and geographical location.

## Goal
The performance goal is twofold:
- Process 20 Type#1s or 30 Type#2s within 10 minutes.
- Process all historical data (800k+ Type#1s, 80k+ Type#2s) within a maximum of 48 hours.

## Data Collection
Data gathering involved meticulous analysis of various tables in the relational database management system (RDBMS). This analysis included understanding the database design, column specifics, and their interconnections. Optimized SQL queries were employed to fetch the data modularly, ensuring both modularity and minimal runtime due to the sheer volume of production data.

### Data Fetched
**Type#1s Master Table:**
- Categorical variables: Type#1 procedure type, Type#1 types, Category#1s
- Numerical variables: Execution deadline days, effective price
- Text variables: Object brief description, country, county, buyer

**Type#2s Master Table:**
- Categorical variables: Type#2 types, Category#1s
- Text variables: Object brief description, country, county, buyer
- Numerical variable: Effective price

## Data Cleaning
Data cleaning was a crucial step involving:
- Removing duplicate values resulting from merged dataframes.
- Addressing missing values, largely handled during SQL data fetching by replacing null values with "No Info".
- Reformatting data types to ensure consistency and readiness for further processing.

## Exploratory Data Analysis (EDA)
EDA encompassed a thorough examination of the dataset, including:
- Type#1s and Type#2s counts per procedure type.
- Type#2s/Buyers counts per price bracket.
- Type#1s counts per price bracket and execution deadline days bracket.
- Distribution of Type#1s and Type#2s across various categories.

## Feature Engineering
### Feature Construction
- **Type#1s/Type#2s Text:** Converted to text embeddings using the Sentence Transformers multi-lingual algorithm.
- **Category#1 Text:** Lemmatized and converted to embeddings using the same algorithm.
- **Type#1 Type:** Categorical feature encoded using the Dummy technique.
- **Procedure Type:** Another categorical feature encoded using the Dummy technique.

### Feature Selection
Feature selection involved an interactive process with multiple trials exploring various combinations and weights to optimize performance hance was a pivotal aspect of the project, aiming to identify the most relevant attributes to optimize performance and accuracy.

### Feature Construction
- **Type#1s/Type#2s Text:** The textual content of Type#1s and Type#2s was converted into dense embeddings using the Sentence Transformers multi-lingual algorithm. This process captured the semantic similarity of the text, enabling effective comparison.
- **Category#1 Text:** Recognizing the importance of Category#1 descriptions, Category#1 text data underwent lemmatization and subsequent conversion into embeddings using the same Sentence Transformers multi-lingual algorithm. This step enhanced the algorithm's ability to capture nuanced similarities between Category#1 categories.
- **Type#1 Type:** The categorical feature representing the type of Type#1 was encoded using the Dummy technique. This transformation facilitated the incorporation of Type#1 type information into the similarity comparison process.
- **Procedure Type:** Similarly, the categorical feature indicating the procedure type was encoded using the Dummy technique. This encoding allowed the algorithm to consider the procedure type as a relevant factor in identifying similar Type#1s and Type#2s.

### Feature Selection Strategies
Feature selection involved a systematic exploration of various feature combinations and weighting strategies to determine the optimal set of attributes for similarity comparison.

### Final Feature Selection
After extensive experimentation and analysis, the final feature selection comprised a weighted combination of key attributes:
- Type#1/Type#2 Text (60% weight)
- Category#1 Text (35% weight)
- Effective Price Bracket (5% weight)

These features, combined with text embeddings generated by the Sentence Transformers multi-lingual algorithm, formed the basis of the similarity comparison algorithm, achieving a balance between semantic relevance and contextual information.

## Modeling
The project employed Approximate Nearest Neighbor (ANN) search models, specifically ANNOY by Spotify, to find top matches for a given query. ANNOY efficiently preprocesses data into an index, speeding up the search process.

## Results
Results were manually analyzed based on reference test data provided. The project aimed to ensure that the top matches for Type#1s/Type#2s were acceptable and intuitive.

## Conclusion
The similarity comparison algorithm successfully identifies top matches for any given Type#1/Type#2. The project achieved its performance and production readiness goals, processing a significant amount of data efficiently. Further enhancements could include additional features or optimizations for even better results.

**Note:** Due to legal constraints, the complete details of the project cannot be disclosed, and certain proprietary information may be omitted from this report.
